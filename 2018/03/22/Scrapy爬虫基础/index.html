<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta name="keywords" content="hexo, autumn">
    <title>
        TT ❤ LL 印迹
    </title>
    <!-- favicon -->
    
    <link rel="icon" href="https://cdn.jsdelivr.net/gh/frontendsophie/hexo-theme-autumn@1.0.0/source/img/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- highlight -->
    <link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.12.0/styles/github-gist.min.css">
    <script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad()
    </script>
    <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/frontendsophie/hexo-infinite-scroll@1.0.0/dist/hexo-infinite-scroll.min.css">
    <script src="https://cdn.jsdelivr.net/gh/frontendsophie/hexo-infinite-scroll@1.0.0/dist/hexo-infinite-scroll.min.js"></script>
    <script>
        infiniteScroll()

        // for mobile menu
        $(function () {
            $('.social-button').click(function () {
                if ($('.social-links').hasClass('hide-links')) {
                    $('.social-links').removeClass('hide-links')
                } else {
                    $('.social-links').addClass('hide-links')
                }
            })
        })
    </script>
</head>

    <body style="background: url(https://cdn.jsdelivr.net/gh/frontendsophie/hexo-theme-autumn@1.0.0/source/img/button-bg.png) #f3f3f3">
        <div class="container">
            <header class="header">
    <h1 class="title">
        <a href="/" class="logo">
            TT ❤ LL 印迹
        </a>
    </h1>
    <h2 class="desc">
        
    </h2>

    <nav class="links">
        <button class="social-button">
            menu
        </button>
        <ul class="social-links hide-links">
            
                <li>
                    <a href="https://github.com/FrontendSophie">
                        Github
                    </a>
                </li>
                
                <li>
                    <a href="https://www.linkedin.com/in/frontendsophie/">
                        LinkedIn
                    </a>
                </li>
                
        </ul>
    </nav>
</header>
                <main class="main">
                    <article class="post">
    
    
    <h4 class="post-cat">
        <a href="/categories/生の术/">
            生の术
        </a>
    </h4>
    
    
    <h2 class="post-title">
        Scrapy爬虫基础篇
    </h2>
    <ul class="post-date">
        <li>
            2018-03-22
        </li>
        <li>
            绛墨铤
        </li>
    </ul>
    <div class="post-content">
        <h1 id="爬虫与Scrapy初识"><a href="#爬虫与Scrapy初识" class="headerlink" title="爬虫与Scrapy初识"></a>爬虫与Scrapy初识</h1><h2 id="何谓爬虫"><a href="#何谓爬虫" class="headerlink" title="何谓爬虫"></a>何谓爬虫</h2><p>网络爬虫，顾名思义，是指在互联网上自动爬取网站信息的程序，又被称为网络蜘蛛或网络机器人。</p>
<p>爬虫步骤：</p>
<ul>
<li>根据网页URL下载网页</li>
<li>提取HTML中的目标信息，存储到数据库中</li>
<li>爬取页面中的URL</li>
</ul>
<h2 id="何谓Scrapy"><a href="#何谓Scrapy" class="headerlink" title="何谓Scrapy"></a>何谓Scrapy</h2><p>Scrapy 是 Python环境下的一个开源网络爬虫框架，详情可见 <a href="https://doc.scrapy.org/en/latest/index.html" target="_blank" rel="noopener">官方文档</a>   </p>
<p>在Anaconda环境下的基本命令，打开Anaconda prompt，以下为安装命令：</p>
<pre><code class="python">conda install scrapy #安装scrapy
scrapy.version_info #查看版本</code></pre>
<p>创建项目</p>
<pre><code class="python">scrapy startproject example #创建example项目
tree example #查看项目目录文件</code></pre>
<p>会生成</p>
<pre><code class="python">project/
    scrapy.cfg        #项目的配置文件
    modules/          #该项目的python模块,之后您将在此加入代码
        __init__.py
        items.py      #项目中的item文件
        pipelines.py  #项目中的pipelines文件
        settings.py   #项目的设置文件
        spiders/      #放置spider代码的目录
            __init__.py
</code></pre>
<h2 id="Scrapy架构概览"><a href="#Scrapy架构概览" class="headerlink" title="Scrapy架构概览"></a>Scrapy架构概览</h2><p><img src="http://scrapy-chs.readthedocs.io/zh_CN/latest/_images/scrapy_architecture.png" alt="Architecture overview"></p>
<p>下表为Scrapy组件及其描述：</p>
<table>
<thead>
<tr>
<th align="center">组件</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Scrapy  Engine</td>
<td align="left">引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。</td>
</tr>
<tr>
<td align="center">调度器(Scheduler)</td>
<td align="left">调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。</td>
</tr>
<tr>
<td align="center">下载器(Downloader)</td>
<td align="left">下载器负责获取页面数据并提供给引擎，而后提供给spider。</td>
</tr>
<tr>
<td align="center">Spiders</td>
<td align="left">Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html#topics-spiders" target="_blank" rel="noopener">Spiders</a> 。</td>
</tr>
<tr>
<td align="center">Item Pipeline</td>
<td align="left">Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。</td>
</tr>
</tbody></table>
<p>运作原理：</p>
<ul>
<li>Spider爬取URL上数据时，使用该URL构造Request提交给Scheduler</li>
<li>Scheduler运算Request对象并传给Downloader</li>
<li>Downloader根据Request对象中的地址向HTTP网站服务器发送请求，服务器返回Response对象</li>
<li>Response对象被送给SPIDER页面解析函数进行处理，页面解析函数提取数据并封装成Item提交给ENGINE，之后被送往ITEM　PIPLINE进行处理，最后由EXPORTER以某种数据写入文件。　　</li>
</ul>
<p>简而言之，爬虫爬取数据的步骤为：提取数据、封装数据、处理数据、导出数据。</p>
<h1 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h1><p>从URL上提取数据的核心技术是HTTP文本解析，Python中常用BeautifulSoup和lxml模块处理此类问题。提取数据要先创建对象，选中数据，然后再进行数据提取。</p>
<h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><h3 id="Selector创建对象"><a href="#Selector创建对象" class="headerlink" title="Selector创建对象"></a>Selector创建对象</h3><p>Selector类的实现位于scrapy.selector模块，创建Selector对象时，可将页面的HTML文档字符串传递给Selector构造器方法的text参数：</p>
<p>Scrapy综合以上量点实现了Selector类，它是基于lxml库构建的，并简化了API借口。在Scrapy中使用Selector对象提取页面数据时，使用时先通过XPath或CSS选择器选中页面中要提取的数据，然后进行提取：</p>
<p>text:</p>
<pre><code class="python">from scrapy.selector import Selector
text=&quot;&quot;&quot;
&lt;html&gt;
    &lt;body&gt;
        &lt;h1&gt;This is  heading1&lt;/h1&gt;
        &lt;h2&gt;This is  heading2&lt;/h2&gt;
        &lt;h3&gt;This is  heading3&lt;/h3&gt;
        &lt;ul&gt;
            &lt;li&gt;Coffee&lt;/li&gt;
            &lt;li&gt;Milk&lt;/li&gt;
            &lt;li&gt;Tea&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
selector = Selector(text=text)
print selector</code></pre>
<h3 id="Response构建对象"><a href="#Response构建对象" class="headerlink" title="Response构建对象"></a>Response构建对象</h3><p>也可以使用Response对象构造Selector对象，将其传递为Selector构造器方法的response参数:</p>
<pre><code class="markdown">from scrapy.selector import Selector
from scrapy.http import HtmlResponse
response = HtmlResponse(url=&#39;http://www.example.com&#39;,body=text,encoding=&#39;utf8&#39;)
selector = Selector(response=response)
print selector</code></pre>
<p>实际使用中，几乎不用手动创建对象，在第一次访问一个Response对象的selector属性时，Response对象内部会以自身为参数自动创建Selector对象，并将该Selector对象缓存，以便下次使用：</p>
<pre><code class="python\">from scrapy.http import HtmlResponse
response = HtmlResponse(url=&#39;http://www.example.com&#39;,body=text,encoding=&#39;utf8&#39;)
print response.selector # 直接使用Response对象的内置Selector</code></pre>
<p><strong>Response对象</strong>介绍</p>
<p>Response对象用来描述一个HTTP响应，在爬取网页时，通常其内容是HTML文本，创建的是HtmlResponse。Response是一个基类，另外有TextResponse、HtmlResponse、XmlResponse三个子类。</p>
<p>HtmlResponse属性及方法：</p>
<pre><code class="python">class HtmlResponse(scrapy.http.response.text.TextResponse):
follow(self, url, callback=None, method=&#39;GET&#39;, headers=None, body=None, cook=None, meta=None, encoding=None, priority=0, dont_filter=False, errback=None)</code></pre>
<p>主要方法：</p>
<ul>
<li>css(self, query)</li>
<li>urljoin(self, url):Join this Response’s url with a possible relative url to form an absolute interpretation of the latter.</li>
<li>xpath(self, query, **kwargs)</li>
</ul>
<h2 id="选中数据"><a href="#选中数据" class="headerlink" title="选中数据"></a>选中数据</h2><p>调用Selector对象的xpath或css方法，返回一个Selectorlist对象，选择文档中某些部分:</p>
<pre><code class="python">selector_list = selector.xpath(&#39;.//h1&#39;)  # 选中文档中所有&lt;h1&gt;
print selector_list # 输出格式为列表
for sel in selector_list: # 迭代访问
    print(sel.xpath(&#39;./text()&#39;))</code></pre>
<p>SelectorList对象也有xpath和css方法，调用他们的行为是：以接收到的参数分别调用其中每一个Selector对象的path或css方法，并将所有结果收集到一个新的SelectorList对象返回给用户。</p>
<pre><code class="python">selector_list.xpath(&#39;./text()&#39;)
selector.xpath(&#39;.//ul&#39;).css(&#39;li&#39;).xpath(&#39;./text()&#39;)</code></pre>
<h3 id="Xpath选中数据"><a href="#Xpath选中数据" class="headerlink" title="Xpath选中数据"></a>Xpath选中数据</h3><p>关于Xpath基本语法可以参考<a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">W3 School Xpath教程</a></p>
<pre><code class="python">from scrapy.selector import Selector
from scrapy.http import HtmlResponse
body=&quot;&quot;&quot;
&lt;html&gt;
    &lt;head&gt;
        &lt;base href=&#39;http://example.com/&#39;/&gt;
        &lt;title&gt;Example website&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div id=&#39;images&#39;&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 1 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 2 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;            
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 3 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 4 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 5 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
        &lt;/div&gt;
     &lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
response=HtmlResponse(url=&#39;http://www.example.com&#39;,body=body,encoding=&#39;utf8&#39;)
response.xpath(&#39;/html&#39;) # 描述一个从根开始的绝对路径
response.xpath(&#39;/html/head&#39;) # z注释同上
response.xpath(&#39;/html/body/div/a&#39;) # E1/E2选中E1子节点中所有的E2
response.xpath(&#39;//a&#39;)# //E:选中文档中所有a
response.xpath(&#39;/html/body//img&#39;)#E1//E2:选中E1后代节点中所有的E2
response.xpath(&#39;//a/text()&#39;) #E/text():选中文档中所有的a的文本
response.xpath(&#39;/html/*&#39;) #E/*:选中E所有元素子节点
response.xpath(&#39;/html/body/div//*&#39;)#选中div所有后代节点
response.xpath(&#39;//div/*/img&#39;) #*/E:选中孙节点中的所有E
response.xpath(&#39;//img/@src&#39;)#E/@ARAR：选中E的所有ATTR属性
response.xpath(&#39;//@href&#39;) #//@ATTR:选中文档中所有的ATTR属性
response.xpath(&#39;//a[1]/img/@*&#39;) #E/@*:选中E的所有属性
response.xpath(&#39;//img/..&#39;)# ..:选中当前节点的父节点，用来描述相对路径
response.xpath(&#39;//a[3]&#39;) #选中a中第三个
response.xpath(&#39;//a[last()]&#39;)
response.xpath(&#39;//a[position&lt;=3]&#39;)
response.xpath(&#39;//div[@id]&#39;) #选中所有含有id属性的div
response.xpath(&#39;//div[@id=&#39;images&#39;]&#39;)</code></pre>
<h3 id="CSS选择器选中数据"><a href="#CSS选择器选中数据" class="headerlink" title="CSS选择器选中数据"></a>CSS选择器选中数据</h3><p>关于CSS的基本语法可以参考<a href="http://www.w3school.com.cn/css/index.asp" target="_blank" rel="noopener">W3 School CSS教程</a>和<a href="https://www.w3.org/TR/selectors-3/" target="_blank" rel="noopener">相关文档</a></p>
<pre><code class="python">from scrapy.selector import Selector
from scrapy.http import HtmlResponse
body=&quot;&quot;&quot;
&lt;html&gt;
    &lt;head&gt;
        &lt;base href=&#39;http://example.com/&#39;/&gt;
        &lt;title&gt;Example website&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div id=&#39;images&#39; atyle=&#39;width:1230px:,&#39;&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 1 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 2 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;            
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 3 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 4 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
            &lt;a href=&#39;images&#39;.html&gt;Name:Image 5 &lt;br/&gt;&lt;img src=&#39;image1.jpg&#39;/&gt;&lt;/a&gt;
        &lt;/div&gt;
     &lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
response=HtmlResponse(url=&#39;http://www.example.com&#39;,body=body,encoding=&#39;utf8&#39;)
response.css(&#39;img&#39;) # 选中所有的img元素
response.css(&#39;base,title&#39;) # 选中所有的base和title
response.css(&#39;div img&#39;) # E1 E2:选中E1后代元素中所有的E2元素
response.css(&#39;body&gt;div&#39;)# 选中body子元素中的div
response.css(&#39;[style]]&#39;)# 选中包含所有style属性的元素
response.css(&#39;[id=images-1]&#39;) #选中属性id值为images-1的元素
response.css(&#39;div&gt;a:nth-child(1)&#39;) #E:nth-child(n):选中E元素,且该元素必须是其父元素的第n个子元素，选中div的第一个a
response.css(&#39;div:nth-child(2)&gt;a:nth-child(1)&#39;)#选中第二个div的第一个a
response.css(&#39;div:first-child&gt;a:last-child&#39;) #E:first-child，选中E元素且该元素是其父元素的第一个子元素，#选中第一个div的最后一个a
response.css(&#39;a::text&#39;)#E::text:选中E元素的文本节点</code></pre>
<h2 id="提取数据-1"><a href="#提取数据-1" class="headerlink" title="提取数据"></a>提取数据</h2><h3 id="Selector提取数据"><a href="#Selector提取数据" class="headerlink" title="Selector提取数据"></a>Selector提取数据</h3><p>调用Selector或Selectorlist对象的以下方法可将选中的内容提取：</p>
<pre><code class="python"># Selector的extract()方法
s1 = selector.xpath(&#39;.//li&#39;)   
print s1
s1[0].extract()
s1 = selector.xpath(&#39;.//li/text()&#39;)
print s1
s1[1].extract()
# Selector/SelectorList的re()方法,另外SelectorList有re_first()
s1 = selector.xpath(&#39;.//li/text()&#39;).re(&#39;正则&#39;) 
# SelectorList的extract()方法，若用extract_first(),则返回列表中第一个Selector对象调用extract方法的结果
s1 = selector.xpath(&#39;.//li/text()&#39;) 
print s1
s1.extract() # 提取对象为列表，返回列表</code></pre>
<p>实际使用中，为方便用户使用，Response对象还提供了xpath和css方法，它们在内部分别调用Selector的xpath和css方法。</p>
<pre><code class="python">response.xpath{&#39;.//h1/text()&#39;}
response.css(&#39;li::text&#39;).extract() # 注意css与xpath语法区别</code></pre>
<h3 id="LinkExtractor提取链接"><a href="#LinkExtractor提取链接" class="headerlink" title="LinkExtractor提取链接"></a>LinkExtractor提取链接</h3><p>Scrapy提供了一个专门提取链接的类LinkExtractor，在提取大量链接或者是链接比较复杂时，更加方便。</p>
<pre><code class="python">from scrapy.linkextractors import LinkExtractor
le = LinkExtractor()  # 如果不传递任何参数，则提取页面的所有链接
links = le.extract_links(responsel)
------------------------------------------------------
pattern = &#39;/intro/.+\.html$&#39; 
le = Linkextractor(allow=pattern) #allow接收正则表达式，提取绝对url与正则表达式匹配的链接，若是该参数为空，则提取全部链接
# le = Linkextractor(deny=pattern) #与allow相反，排除url与正则表达式匹配的链接
links = le.extract_links(responsel)
------------------------------------------------------
domains=[&#39;github.com&#39;,&#39;stackoverflow.com&#39;]
le = LinkExtractors(allow_domains=domains) # allow_domains接收一个指定的域名或列表，提取到指定域的链接
# deny_domains,与allow相反
links = le.extract_links(responsel)
------------------------------------------------------
le = LinkExtractor(restrict_xpaths=&#39;//div[@id=&quot;top&quot;]&#39;) #restrict_xpaths接收一个XPath表达式或表达式列表，提取选中区域下的链接
# le = LinkExtractor(restrict_css=&#39;div#id=&#39;bottom&#39;) #restrict_css接收css选择器...
links = le.extract_links(responsel)
------------------------------------------------------
le = LinkExtractor(tags=&#39;script&#39;,attr=&#39;src&#39;) # tags接收一个标签或一个标签列表，提取指定标签内的链接，默认为[&#39;a&#39;,&#39;area&#39;];attr接收一个属性或是属性列表，提取属性内的指定链接，默认为&#39;href&#39;
links = le.extract_links(responsel)
------------------------------------------------------
process_value:接收一个回调函数</code></pre>
<h1 id="封装和处理数据"><a href="#封装和处理数据" class="headerlink" title="封装和处理数据"></a>封装和处理数据</h1><h2 id="封装数据"><a href="#封装数据" class="headerlink" title="封装数据"></a>封装数据</h2><p> Scrapy提取数据后，需要将这些零散的数据进行封装，便于处理和传递。</p>
<h3 id="Item和Field"><a href="#Item和Field" class="headerlink" title="Item和Field"></a>Item和Field</h3><p>Scrapy提供了以下两个类，用户可以使用它们自定义数据类，封装爬取到的数据：</p>
<ul>
<li>Item基类：自定义数据的基类，如BookItem</li>
<li>Field类：用来描述自定义数据类包含哪些字段（如name、price）</li>
</ul>
<p>自定义数据类只需继承Item，并创建一系列Field对象的类属性。以定义书籍信息BookItem为例，它包含了两个字段，分别为书的名字name和书的价格price，代码如下：</p>
<pre><code class="python">from scripy import Item,Field
class BookItem(Item):
    name = Field()
    price = Field()</code></pre>
<p>Item支持字典对象，因此BookItem在使用上和Python字典类似，可以按以下方式创建BookItem对象：</p>
<pre><code class="python">book1 = BookItem(name=&#39;Python&#39;,price=40)
book2 = BookItem()
book2[&#39;name&#39;] = &#39;Life of Pi&#39;
book2[&#39;price&#39;] = 30
book1[&#39;name&#39;]
list(book1.items())</code></pre>
<p>有些时候我们需要根据自己的自定义数据类（Item子类）进行扩展。例如在example项目中有添加了一个新的Spider，它负责在另外的网站爬取国外书脊的数据，此类数据比之前多了一个译者字段，此时可以继承BookItem定义的一个ForeignBookItem类，在其中添加一个一折字段，代码如下：</p>
<pre><code class="python">class ForignBookItem(BookItem):
    translator = Field()
book2[&#39;name&#39;] = &#39;Life of Pi&#39;
book2[&#39;price&#39;] = 30
book2[&#39;translator&#39;] = &#39;某某某&#39;</code></pre>
<h3 id="Field元数据"><a href="#Field元数据" class="headerlink" title="Field元数据"></a>Field元数据</h3><p>一项数据由Spider提交给Scrapy引擎后，可能会被递送给其他组件（Item Pipline、Exporter）处理。假设想传递额外信息给处理数据的某个组件（例如，告诉该组件应以怎样的方式处理数据），此时可以使用Field的元数据。</p>
<pre><code class="python">class ExampleItem(Item):
    x = Field{a=&#39;hello&#39;,b=[1,2,3]) #x包含两个元数据，字符串和列表
    y = Field(a=lambdax:x**2) # y有一个元数据，为函数</code></pre>
<p>访问ExampleItem对象的fields属性，将得到一个包含所有Field对象的字典。</p>
<pre><code class="python">class BookItem(Item):
    authors = Field(serializer=lambda x:&#39;|&#39;.join(x))
# 元数据的键是serializer是CsvItemExporter规定好的，它会用该键获取元数据，即一个串行化函数的对象，并使用这个串行化函数将authors字段串行化成一个字符串。</code></pre>
<h2 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h2><p><strong>Item Pipline</strong></p>
<p>在Scrapy中，Item是处理数据的组件，一个Item Pipline就是一个包含特定接口的类，通常只负责一种功能的数据处理，在一个项目中可以同时启用多个Item Pipline，它们按指定次序级联立起来，形成一条数据流水线。</p>
<h3 id="清洗数据"><a href="#清洗数据" class="headerlink" title="清洗数据"></a>清洗数据</h3><p>在创建一个Scrapy项目时，会自动生成一个piplines.py,它用来放置用户自定义的Item Pipline，在example项目的piplines.py中实现:</p>
<pre><code class="python">class ToscrapeBookPipeline(object):
    #exchange_rate= 8
    def process_item(self, item, spider): 
        # price =  flaot(item[&#39;price&#39;][1:])*self.exchange_rate
        #item[&#39;price&#39;]=￥%.2f%price        
        return item </code></pre>
<p>一个Item Pipline必须实现一个process(item,spider)方法，不用继承特定的类,item是指爬取到的数据，spider指爬取到数据的Spider对象。Item Pipline常用方法：</p>
<ul>
<li>process_item(item,spider):处理某项item时返回一项数据，拖过在处理某项item时会抛出(raise)一个DropItem异常(scrapy.exception.DropItem),该项item便会被抛弃，不再递送给后面的Item Pipline 继续处理，也不会导出到文件。通常，我们在检测到无效的数据或想要过滤掉的数据时会抛出异常。</li>
<li>open_spider(self,spider):Spider打开时（处理数据前）回调该方法，通常该方法用于在开始处理数据之前完成某些初始化的工作，如连接数据可等。</li>
<li>close(self,spider):Spider关闭时（处理数据后）回调该方法，通常该方法用于在处理完所有数据之后完成某些清理工作，如关闭数据库。</li>
<li>from_crawler(cls,crawler):创建Item Pipline对象回调该类方法。通常，在该方法中通过crawler.settings读取配置，根据配置创建Item Pipline对象。</li>
</ul>
<p>在Scrapy中，Item Pipline是可选组件，想要启用某个（或某些）Item Popline，需要在配置文件settings.py进行配置：</p>
<pre><code class="python">ITEM_PIPELINES = {
    &#39;toscrape_book.pipelines.ToscrapeBookPipeline&#39;: 300,
}</code></pre>
<h3 id="数据去重"><a href="#数据去重" class="headerlink" title="数据去重"></a>数据去重</h3><p>为了确保爬取的书籍信息中没有重复项，可以实现一个去重Item Pipline：</p>
<pre><code class="python">from scrapy.exceptions import Dropline
class DuplicatesPipline(object):
    def _init_(self):
        self.book_set = set()
    det process_item(self,item,spider):
        name = item[&#39;name&#39;]
        if name in self.book_set:
            raise DropItem(&quot;Duplicate book found:%s&quot;%item)

        self.book_set.add(name)
        return item #先取出item的name字段，检查书名是否已在集合book_set中，如果存在，就是重复数据，抛出DropItem，将Item抛弃；否则，将item的name字段存入集合，返回item</code></pre>
<p>然后在配置文件settings.py中启用DuplicatesPipline:</p>
<pre><code class="python">ITEM_PIPELINES = {
    &#39;toscrape_book.pipelines.ToscrapeBookPipeline&#39;: 300,
    &#39;toscrape_book.pipelines.DuplicatesPipeline&#39;: 350,
}</code></pre>
<h3 id="将数据存入MongoDB"><a href="#将数据存入MongoDB" class="headerlink" title="将数据存入MongoDB"></a>将数据存入MongoDB</h3><p>如果想把数据存入某种数据库中，可以实现Item Pipline完成此类任务。下面为将数据存入MongoDB数据库的Item Pipline:</p>
<pre><code class="python">from scrapy.item import Item
import pymongo
class MongoDBPipline(object):
    #在类属性中定义两个常量，DB_URI:数据库的URI地址，DB_NAME:数据库的名字
    DB_URI = &#39;mongodb://localhost:27017/&#39;
    DB_NAME = &#39;scrapy_data&#39;
    #在整个爬取过程中，数据库的关闭和连接只进行一次，应在开始之前连接数据库，并在处理完所有数据之后关闭数据。
    def open_spider(self,spider):
        self.client = pymongo.MongoClient(self.DB_URI)
        self.db = self.client[self.DB_URI]

    def close_spider(self,spider):
        self.client.close()
    #在process_iten中实现MongoDB数据库的写入操作，使用self.db和spider.name获取一个集合(collection)然后将数据插入该集合，集合对象的insert_one方法需要传入一个字典对象（不能传入Item对象），因此在调用前先对item的类型进行判断，如果item是Item对象，就将其转化为字典。
    def process_item(self,item,spider):
        collection = self.db[spider.name]
        post= dict(item) if isinstance(item,Item) else item
        collection.insert_one(post)
        return item</code></pre>
<p>接下来测试MongoDBPipline，将配置文件settings.py中启用MongoDBPipline:</p>
<pre><code class="python">ITEM_PIPELINES = {    &#39;toscrape_book.pipelines.ToscrapeBookPipeline&#39;: 300,    &#39;toscrape_book.pipelines.MongoDBPipeline&#39;: 400,
 }</code></pre>
<h1 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h1><p>在Scrapy中，导出数据需要用Exporter（导出器），Scrapy内部实现了多个Exporter，每个Exporter实现一种数据格式的导出，支持数据格式有：</p>
<ol>
<li>JSON（JsonItemExporter）</li>
<li>JSON lines（JsonLinesItemExporter）</li>
<li>CSV（CsvItemExporter）</li>
<li>XML(XmlItemExporter)</li>
<li>Pickle（PickleItemExporter）</li>
<li>Marshal（MarshalItemExporter）</li>
</ol>
<p>在导出数据时，需向Scrapy爬虫提供：文件导出格式(即选用哪个Exporter)及导出路径。可以通过两种方式指定爬虫如何导出数据：命令行参数指定和配置文件指定。</p>
<h2 id="命令行参数"><a href="#命令行参数" class="headerlink" title="命令行参数"></a>命令行参数</h2><p>在运行scrapy crawl命令式时，可以分别使用-o和-t参数指定导出文件路径以及导出数据格式。</p>
<pre><code class="python">scrapy crawl books -t csv -o books1.data
scrapy crawl books -t json -o books1.data
scrapy crawl books -t xml -o books1.data</code></pre>
<p>运行以上命令后，scrapy爬虫会以-t参数中的数据格式字符串（如csv、json、xml）为键，在配置字典FEED_EXPORTERS中搜索Exporter,FEED_EXPORTERS的内容由两个字典的内容合并而成：默认配置文件中的FEED_EXPORTERS_BASE，用户配置文件中的FEED_EXPORTERS。其中内核包含内部支持的导出数据格式，后者包含用户自定义的导出数据格式。如果用户添加了新的导出数据格式（即实现了新的Exporter），可在配置文件settings.py中定义FEED_EXPORTERS，例如：</p>
<blockquote>
<p>Feed_Exporters = {‘excel’:’my_project.my_exporters.ExcelItemExporter’}</p>
</blockquote>
<p>另外，指定文件到处路径时，还可以使用%(name)s和%(time)s两个特殊变量：</p>
<p>%(name)s：会被替换为Spider的名字，%(time)s:会被替换为文件创建时间。</p>
<blockquote>
<p>scrapy crawl books -o ‘exporter_data/%(name)s/%(time)s.csv’</p>
</blockquote>
<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>在配置文件中导出数据：</p>
<pre><code class="python">FEED_URI=&#39;export_data/%(name)s.data&#39; # 导出文件路径
FEED_FORMAT=&#39;csv&#39; # 导出数据格式
FEED_EXPORT_ENCODING=&#39;gbk&#39; # 导出文件编码(默认为json编码，其他为utf-8编码)
FEED_EXPORT_FIELDS=[&#39;name&#39;,&#39;author&#39;,&#39;price&#39;] #导出数据包含的字段（默认包含所有字段）
FEED_EXPORTERS={&#39;excel&#39;:&#39;my_project.my_exporters.ExcelItemExporter&#39;} # 用户自定义Exporter字典，添加新的导出数据格式时使用。</code></pre>
<h2 id="添加自定义导出数据格式"><a href="#添加自定义导出数据格式" class="headerlink" title="添加自定义导出数据格式"></a>添加自定义导出数据格式</h2><p>Scrapy内部的Exporter类在scrapy.exporters模块中实现，以下为其中的代码片段：</p>
<pre><code class="python">class BaseItemExporter(object):

    def __init__(self, **kwargs):
        self._configure(kwargs)

    def _configure(self, options, dont_fail=False):
        &quot;&quot;&quot;Configure the exporter by poping options from the ``options`` dict.
        If dont_fail is set, it won&#39;t raise an exception on unexpected options
        (useful for using with keyword arguments in subclasses constructors)
        &quot;&quot;&quot;
        self.encoding = options.pop(&#39;encoding&#39;, None)
        self.fields_to_export = options.pop(&#39;fields_to_export&#39;, None)
        self.export_empty_fields = options.pop(&#39;export_empty_fields&#39;, False)
        self.indent = options.pop(&#39;indent&#39;, None)
        if not dont_fail and options:
            raise TypeError(&quot;Unexpected options: %s&quot; % &#39;, &#39;.join(options.keys()))

    def export_item(self, item):
        raise NotImplementedError

    def serialize_field(self, field, name, value):
        serializer = field.get(&#39;serializer&#39;, lambda x: x)
        return serializer(value)

    def start_exporting(self):
        pass

    def finish_exporting(self):
        pass

    def _get_serialized_fields(self, item, default_value=None, include_empty=None):
        &quot;&quot;&quot;Return the fields to export as an iterable of tuples
        (name, serialized_value)
        &quot;&quot;&quot;
        if include_empty is None:
            include_empty = self.export_empty_fields
        if self.fields_to_export is None:
            if include_empty and not isinstance(item, dict):
                field_iter = six.iterkeys(item.fields)
            else:
                field_iter = six.iterkeys(item)
        else:
            if include_empty:
                field_iter = self.fields_to_export
            else:
                field_iter = (x for x in self.fields_to_export if x in item)

        for field_name in field_iter:
            if field_name in item:
                field = {} if isinstance(item, dict) else item.fields[field_name]
                value = self.serialize_field(field, field_name, item[field_name])
            else:
                value = default_value

            yield field_name, value

# json
class JsonItemExporter(BaseItemExporter):

    def __init__(self, file, **kwargs):
        self._configure(kwargs, dont_fail=True)
        self.file = file
        # there is a small difference between the behaviour or JsonItemExporter.indent
        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent
        # the addition of newlines everywhere
        json_indent = self.indent if self.indent is not None and self.indent &gt; 0 else None
        kwargs.setdefault(&#39;indent&#39;, json_indent)
        kwargs.setdefault(&#39;ensure_ascii&#39;, not self.encoding)
        self.encoder = ScrapyJSONEncoder(**kwargs)
        self.first_item = True

    def _beautify_newline(self):
        if self.indent is not None:
            self.file.write(b&#39;\n&#39;)

    def start_exporting(self):
        self.file.write(b&quot;[&quot;)
        self._beautify_newline()

    def finish_exporting(self):
        self._beautify_newline()
        self.file.write(b&quot;]&quot;)

    def export_item(self, item):
        if self.first_item:
            self.first_item = False
        else:
            self.file.write(b&#39;,&#39;)
            self._beautify_newline()
        itemdict = dict(self._get_serialized_fields(item))
        data = self.encoder.encode(itemdict)
        self.file.write(to_bytes(data, self.encoding))

# json lines
class JsonLinesItemExporter(BaseItemExporter):
    ...</code></pre>
<ul>
<li>rexport_item(self,item):负责导出爬取到的每一项数据，参数item为一项爬取到的数据，每个子类必须实现该方法。</li>
<li>start_exporting(self):在导出开始时被调用，可在该方法中执行某些初始化工作</li>
<li>finish_exporting(self):在到处完成时被调用，可在该方法中执行某些清理工作</li>
</ul>
<p>以JsonItemExporter为例：</p>
<ul>
<li>为了使最终的导出结果是一个json中的列表，在start_exporting和finish_exporting方法中分别向文件中写入b”[\n,b”\n]”。</li>
<li>在export_item方法中，调用self.encoder.encode方法将一项数据转换成json串，然后写入文件。</li>
</ul>
<p>参照以上源码，可以实现一个能将数据以Excel格式导出的Exporter。在项目中创建一个my_exporters.py（与settings.py同级目录），在其中实现ExcelItemExport:</p>
<pre><code class="python">from scrapy.exporters import BaseItemExporter
import xlwt # 使用第三方库xlwt将数据写入Excel文件
class ExcelItemExporter(BaseItemExporter):

    def __init__(self, file, **kwargs):
        self._configure(kwargs)
        self.file = file
        # 在构造器方法中创建Workbook对象和Worksheet对象，并初始化用           来记录写入行坐标的self.row
        self.wbook = xlwt.Workbook()
        self.wsheet = self.wbook.add_sheet(&#39;scrapy&#39;)
        self.row = 0

    # finish_exporting 方法在所有数据都被写入Excel表格后被调用，在       该方法中调用self.wbook.save方法将Excel表格写入Excel文件。
    def finish_exporting(self):
        self.wbook.save(self.file)

    # 在export_item方法中调用基类的_get_serialized_fields方法，获       得item所有字段的迭代器，然后调用self.wsheet.write方法将各自段       写入Excel表格。
    def export_item(self, item):
        fields = self._get_serialized_fields(item)
        for col,v in enumerate(x for _,x in fields):
            self.wsheet.write(self.row,col,v)
        self.row += 1</code></pre>
<p>完成ExcelItemExporter后，在配置文件settings.py中添加如下代码：</p>
<blockquote>
<p>FEED_EXPORTERS=｛’excel’:’example.my_exports.ExcelItemExporter’｝</p>
</blockquote>
<p>现在可以使用ExcelItemExporter导出数据了，以-t excel -o books.xls</p>
<blockquote>
<p>scrapy crawl books -t excel -o books.xls</p>
</blockquote>

    </div>
</article>
                </main>
                <aside class="aside">
                    <section class="aside-section">
                        
    <h1>Categories</h1>

    <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/生の术/">生の术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/生の迹/">生の迹</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/生の道/">生の道</a></li></ul>

                    </section>
                    <section class="aside-section">
                        
    <h1>Archives</h1>

    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archiveshl/2018/">2018</a></li></ul>


                    </section>
                    <section class="aside-section tag">
                        
    <h1>Tags</h1>

    <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/">Markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Oracle/">Oracle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VBA/">VBA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据科学/">数据科学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自言语/">自言语</a></li></ul>

                    </section>
                </aside>
        </div>
    </body>

</html>